{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Tokenization with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    with open('./article/art1.dat','r') as article:\n",
    "        text = article.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(string.punctuation)\n",
    "        tokens = nltk.word_tokenize(no_punctuation)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Counter.most_common of Counter({'the': 9, ',': 6, 'to': 4, 'a': 4, 'ringgit': 3, 'in': 3, 'on': 3, '.': 3, '%': 3, 'us': 2, 'death-cross': 2, 'pattern': 2, 'has': 2, 'this': 2, 'previous': 2, 'took': 2, 'dollar': 2, 'of': 2, '3': 2, 'from': 2, 'trading': 2, 'strengthen': 1, 'against': 1, 'dollar.+the': 1, 'dollar-ringgit': 1, 'exchange': 1, 'rate': 1, 'is': 1, 'forming': 1, 'which': 1, 'past': 1, 'led': 1, 'decline': 1, 'currency': 1, 'pair': 1, 'based': 1, 'technical': 1, 'charts': 1, '+bloomberg': 1, 'reported': 1, 'wednesday': 1, 'that': 1, 'occurs': 1, 'when': 1, '50-day': 1, 'moving': 1, 'average': 1, 'drops': 1, 'below': 1, '100-day': 1, 'gauge': 1, '+it': 1, 'said': 1, 'three': 1, 'occasions': 1, 'move': 1, 'place': 1, 'posted': 1, 'additional': 1, 'losses': 1, 'and': 1, '7': 1, 'before': 1, 'finding': 1, 'bottom': 1, '+the': 1, 'underperformed': 1, 'asian': 1, 'currencies': 1, 'since': 1, 'policy': 1, 'makers': 1, 'steps': 1, 'november': 1, 'deter': 1, 'foreign': 1, 'banks': 1, 'non-deliverable': 1, 'forwards': 1, 'wire': 1, 'report': 1, 'said.+at': 1, '1.45pm': 1, 'was': 1, 'at': 1, '4.4305': 1, 'close': 1, '4.4312.+': 1})>\n"
     ]
    }
   ],
   "source": [
    "tokens = get_tokens()\n",
    "count = Counter(tokens)\n",
    "print(count.most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Stop words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 6), ('ringgit', 3), ('.', 3), ('%', 3), ('us', 2), ('death-cross', 2), ('pattern', 2), ('previous', 2), ('took', 2), ('dollar', 2), ('3', 2), ('trading', 2), ('strengthen', 1), ('dollar.+the', 1), ('dollar-ringgit', 1), ('exchange', 1), ('rate', 1), ('forming', 1), ('past', 1), ('led', 1), ('decline', 1), ('currency', 1), ('pair', 1), ('based', 1), ('technical', 1), ('charts', 1), ('+bloomberg', 1), ('reported', 1), ('wednesday', 1), ('occurs', 1), ('50-day', 1), ('moving', 1), ('average', 1), ('drops', 1), ('100-day', 1), ('gauge', 1), ('+it', 1), ('said', 1), ('three', 1), ('occasions', 1), ('move', 1), ('place', 1), ('posted', 1), ('additional', 1), ('losses', 1), ('7', 1), ('finding', 1), ('bottom', 1), ('+the', 1), ('underperformed', 1), ('asian', 1), ('currencies', 1), ('since', 1), ('policy', 1), ('makers', 1), ('steps', 1), ('november', 1), ('deter', 1), ('foreign', 1), ('banks', 1), ('non-deliverable', 1), ('forwards', 1), ('wire', 1), ('report', 1), ('said.+at', 1), ('1.45pm', 1), ('4.4305', 1), ('close', 1), ('4.4312.+', 1)]\n"
     ]
    }
   ],
   "source": [
    "tokens = get_tokens()\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "count = Counter(filtered)\n",
    "print(count.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Stemming with Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 6), ('ringgit', 3), ('.', 3), ('%', 3), ('us', 2), ('death-cross', 2), ('pattern', 2), ('currenc', 2), ('report', 2), ('move', 2), ('previou', 2), ('took', 2), ('dollar', 2), ('3', 2), ('trade', 2), ('strengthen', 1), ('dollar.+th', 1), ('dollar-ringgit', 1), ('exchang', 1), ('rate', 1), ('form', 1), ('past', 1), ('led', 1), ('declin', 1), ('pair', 1), ('base', 1), ('technic', 1), ('chart', 1), ('+bloomberg', 1), ('wednesday', 1), ('occur', 1), ('50-day', 1), ('averag', 1), ('drop', 1), ('100-day', 1), ('gaug', 1), ('+it', 1), ('said', 1), ('three', 1), ('occas', 1), ('place', 1), ('post', 1), ('addit', 1), ('loss', 1), ('7', 1), ('find', 1), ('bottom', 1), ('+the', 1), ('underperform', 1), ('asian', 1), ('sinc', 1), ('polici', 1), ('maker', 1), ('step', 1), ('novemb', 1), ('deter', 1), ('foreign', 1), ('bank', 1), ('non-deliver', 1), ('forward', 1), ('wire', 1), ('said.+at', 1), ('1.45pm', 1), ('4.4305', 1), ('close', 1), ('4.4312.+', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(count.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: tf-idf with Scikit-learn (combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from collections import defaultdict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/muhdlaziem/Workspace/NLP/Week8/article'\n",
    "token_dict ={}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/muhdlaziem/Workspace/NLP/Week8/article/art4.dat\n",
      "/home/muhdlaziem/Workspace/NLP/Week8/article/art3.dat\n",
      "/home/muhdlaziem/Workspace/NLP/Week8/article/art1.dat\n",
      "/home/muhdlaziem/Workspace/NLP/Week8/article/art2.dat\n",
      "/home/muhdlaziem/Workspace/NLP/Week8/article/art5.dat\n",
      "/home/muhdlaziem/Workspace/NLP/Week8/article/art6.dat\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems  = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path =  subdir + os.path.sep + file\n",
    "        print(file_path)\n",
    "        article = open(file_path,'r')\n",
    "        text = article.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(string.punctuation)\n",
    "        token_dict[file] = no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muhdlaziem/Workspace/mws/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform((token_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster: \n",
      "Cluster 0: \n",
      " ,\n",
      " 's\n",
      " wa\n",
      " feder\n",
      " kill\n",
      " servic\n",
      " hi\n",
      " unit\n",
      " ibrahimov\n",
      " game\n",
      "Cluster 1: \n",
      " trade\n",
      " ,\n",
      " %\n",
      " ringgit\n",
      " $\n",
      " billion\n",
      " high\n",
      " deficit\n",
      " thi\n",
      " death-cross\n"
     ]
    }
   ],
   "source": [
    "def k_means(tfs):\n",
    "    true_k=2\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=50,n_init=1)\n",
    "    model.fit(tfs)\n",
    "    print(\"Top terms per cluster: \")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf.get_feature_names()\n",
    "    \n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d: \" % i)\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind])\n",
    "k_means(tfs)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
